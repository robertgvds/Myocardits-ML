{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8733110,"sourceType":"datasetVersion","datasetId":5242137},{"sourceId":8740562,"sourceType":"datasetVersion","datasetId":5242143,"isSourceIdPinned":true}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/robertgvds/treinamento-myocarditis?scriptVersionId=186890881\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# CODIGO DO TREINAMENTO USANDO QUALQUER REDE","metadata":{}},{"cell_type":"code","source":"#------------------------------------------------------------------------------\n# CONSTANTES\n\nDATASET_TYPE = ['cleaned','limited-to-five', 'limited-to-ten', 'selected']\nDATASET_TYPE = DATASET_TYPE[1] # Dataset utilizado no treinamento\n\nCNN_MODEL = ['rede-inicial', 'kcl-cnn']\nCNN_MODEL = CNN_MODEL[1] # Rede utilizada no treinamento\n\nSEED = 10\nnp.random.seed(SEED) # semente geradora dos numeros aleatorios\nrandom.seed(SEED)\ntf.random.set_seed(SEED)\n\nN_FOLDS = 5\nN_EPOCHS = 70\nBATCH_SIZE = 32\nTARGET_SIZE = (100, 100)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['TF_DISABLE_JIT'] = '1'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!jupyter notebook --NotebookApp.iopub_msg_rate_limit=1.0e10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bibliotecas e Constantes","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow[and-cuda]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#------------------------------------------------------------------------------\n# BIBLIOTECAS\n\nimport os\nimport numpy as np\nfrom PIL import Image\nimport sys\nimport random\n\n# BIBLIOTECAS DEEP LEARNING\nimport datetime\nimport tensorflow as tf\nfrom sklearn.metrics import auc, classification_report, confusion_matrix, roc_curve\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import CSVLogger\nfrom tensorflow.keras.optimizers import AdamW","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Diretórios e Carregamentos de Dados","metadata":{}},{"cell_type":"code","source":"# Criando pasta de resultados\nif not os.path.exists(f'{DATASET_TYPE}'):\n    os.mkdir(f'{DATASET_TYPE}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#------------------------------------------------------------------------------\n# DIRETÓRIOS DOS DATASETS\n\nDATASET_PATH = f'/kaggle/input/myocardits-dataset-{DATASET_TYPE}'\nRESULTS_PATH = f'/kaggle/working/{DATASET_TYPE}'\n\nNORMAL_PATH = DATASET_PATH + '/Normal/'\nSICK_PATH = DATASET_PATH + '/Sick/'\n\n# Diretorios de cada Individuo:\nnormal_datasets = [f'{NORMAL_PATH}Individuo_{i:02}/' for i in range(1, 17)]\nsick_datasets = [f'{SICK_PATH}Individuo_{i:02}/' for i in range(17, 48)]\n\nprint(normal_datasets)\nprint(sick_datasets)\n\nNORMAL_SPLITS = [[9, 10, 12, 15],\n                 [1,8,11],\n                 [4,5,14],\n                 [2,7,13],\n                 [3,6,16]]\n\n\nSICK_SPLITS = [[21,23,26,27,36,38],\n               [18,37,39,40,41,45,46],\n               [20,24,28,29,31,32],\n               [19,22,30,33,42,47],\n               [17,25,34,35,43,44]]\n'''\nSICK_SPLITS = [[23,26,27,36],\n               [37,39,40,41,45],\n               [24,28,29,31],\n               [22,30,33,42],\n               [17,25,34]]\n'''\n\nnormal_splits = [[],[],[],[],[]]\nsick_splits = [[],[],[],[],[]]\n\nfor split in range(N_FOLDS):\n    normal_splits[split].extend(normal_datasets[i-1] for i in NORMAL_SPLITS[split])\n    sick_splits[split].extend(sick_datasets[i-17] for i in SICK_SPLITS[split])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#------------------------------------------------------------------------------\n# CARREGAMENTO DE DADOS\n\nprint('\\nIniciando carregamento e processamento das imagens..............!')\n\ndef carregar_imagens(diretorio):\n    global num_imagens\n    imagens = []\n    for pasta_atual, subpastas, arquivos in os.walk(diretorio):\n        for arquivo in arquivos:\n            if arquivo.endswith(('.jpg', '.jpeg', '.png')):\n                caminho = os.path.join(pasta_atual, arquivo)\n                \n                img = Image.open(caminho)\n                img = img.resize(TARGET_SIZE)\n                img_array = np.array(img)\n                imagens.append(img_array)\n                \n                num_imagens += 1\n                sys.stdout.write(\"\\rNumero de imagens carregados: %i\" % num_imagens)\n                sys.stdout.flush()\n                \n    return imagens\n\nnum_imagens = 0\n\nprint('\\nPacientes normais:')\nnormal_groups = []\nfor diretorios in normal_splits:\n    imagens = []\n    for individuos in diretorios:\n        imagens.extend(carregar_imagens(individuos))\n    normal_groups.append(imagens)\n\nnum_imagens = 0\n\nprint('\\n\\nPacientes doentes:')\nsick_groups = []\nfor diretorios in sick_splits:\n    imagens = []\n    for individuos in diretorios:\n        imagens.extend(carregar_imagens(individuos))\n    sick_groups.append(imagens)\n\n# DATASETS SEPRADAOS EM 5 PARA VALIDAÇÃO CRUZADA    \nx_data = [[],[],[],[],[]]\ny_data = [[],[],[],[],[]]\n\nprint('\\n\\nNumero de imagens por split:')\nfor i in range(N_FOLDS):\n    x_data[i].extend(path for path in normal_groups[i])\n    y_data[i].extend([1, 0] for path in normal_groups[i])\n    x_data[i].extend(path for path in sick_groups[i])\n    y_data[i].extend([0, 1] for path in sick_groups[i])\n    print(f'Split {i+1}: {len(x_data[i])} imagens ({len(normal_groups[i])} saudáveis e {len(sick_groups[i])} doentes).')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Treinamento","metadata":{}},{"cell_type":"code","source":"# Arquitetura CNN\nif CNN_MODEL == 'rede-inicial':\n    def cnn_model(input_shape, num_classes):\n    model=Sequential()\n    model.add(Conv2D(32,3,padding='same',activation='relu',strides=2,input_shape=input_shape))\n    model.add(Conv2D(64,3,padding='same',activation='relu',strides=2))\n    model.add(Conv2D(128,3,padding='same',activation='relu',strides=2))\n    model.add(Conv2D(256,3,padding='same',activation='relu',strides=1))\n    model.add(Conv2D(256,3,padding='same',activation='relu',strides=1))\n    model.add(Conv2D(256,3,padding='same',activation='relu',strides=1))\n    model.add(Flatten())\n    model.add(Dense(256,activation='relu'))\n    model.add(Dense(128,activation='relu'))\n    model.add(Dense(64,activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(num_classes,activation='softmax'))\n    return model\n\nif CNN_MODEL == 'kcl-cnn':\n    def cnn_model(input_shape, num_classes):\n        model=Sequential()\n        model.add(Conv2D(32, 3, activation='relu', input_shape=input_shape))\n        model.add(MaxPooling2D())\n        model.add(Dropout(0.2))\n        model.add(Conv2D(64, 3, activation='relu'))\n        model.add(MaxPooling2D())\n        model.add(Dropout(0.2))\n        model.add(Conv2D(64, 3, activation='relu'))\n        model.add(MaxPooling2D())\n        model.add(Dropout(0.2))\n        model.add(Flatten())\n        model.add(Dense(100, activation='relu'))\n        model.add(Dropout(0.2))\n        model.add(Dense(num_classes, activation='softmax'))\n        return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#------------------------------------------------------------------------------\n# SEPARAÇÃO DE DADOS E TREINAMENTO\n\nlst_accuracy=[]\nlst_accuracy_v=[]\nlst_acc=[]\nlst_loss=[]\nlst_reports=[]\nlst_AUC=[]\nlst_matrix=[]\nlst_matrix_v=[]\nlst_times=[]\nlst_history=[]\n\n#------------------------------------------------------------------------------\n# TREINAMENTO POR FOLDS\n\nfor fold in range(N_FOLDS):\n\n    print(f'\\n\\nFOLD {fold+1}:')\n\n    #--------------------------------------------------------------------------\n    # CARREGAMENTO DAS IMAGENS DE TREINAMENTO, VALIDACAO E TESTE\n\n    print(f'\\nCarregamento das imagens do fold {fold+1} para treinamento.............!')\n\n    folds = [0, 1, 2, 3, 4]\n\n    x_test = np.array(x_data[fold])\n    y_test = np.array(y_data[fold])\n    folds.remove(fold)\n\n    x_valid = np.array(x_data[folds[0]])\n    y_valid = np.array(y_data[folds[0]])\n    folds.remove(folds[0])\n\n    x_train, y_train = [], []\n    for i in folds:\n        x_train.extend(x_data[i])\n        y_train.extend(y_data[i])\n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n\n    print(f'Numero de imagens no treinamento: {len(x_train)} imagens.')\n    print(f'Numero de imagens na validação: {len(x_valid)} imagens.')\n    print(f'Numero de imagens no teste: {len(x_test)} imagens.')\n\n    #--------------------------------------------------------------------------\n    # ARQUITETURA E COMPILACAO\n\n    model=cnn_model((100, 100, 1), 2)\n    \n    OPTIMIZER = AdamW(weight_decay = 0.004)\n    \n    # Compilacao do modelo\n    model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['categorical_accuracy'])\n\n    #--------------------------------------------------------------------------\n    # TREINAMENTO\n    print('\\nIniciando o treinamento.........................................!\\n')\n\n    calback=CSVLogger(RESULTS_PATH + f'/logger_fold{fold+1}.log')\n\n    # Treinando o modelo\n    start=datetime.datetime.now()\n\n    history=model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=N_EPOCHS, validation_data=(x_valid, y_valid), callbacks=[calback])\n\n    end=datetime.datetime.now()\n    training_time=end-start\n\n    # Salvamento do modelo\n    model.save(RESULTS_PATH + f'/{DATASET_TYPE}-CNN-{fold+1}.h5')\n\n    #--------------------------------------------------------------------------\n    # TESTE\n\n    # Testando o modelo\n    print(\"\\nTestando imagens................................................!\\n\")\n\n    # Acuracia e Perda do Teste\n    test_loss, test_acc = model.evaluate(x_test, y_test)\n\n    print(model.metrics_names)\n\n    #--------------------------------------------------------------------------\n    # ARMAZENAMENTO DOS INFORMACOES\n\n    # Fazendo previsões\n    predicts = model.predict(x_test)\n    predicts = predicts.argmax(axis=1)\n\n    # Obtendo os rótulos verdadeiros\n    actuals=y_test.argmax(axis=1)\n\n    # Calculando a curva ROC\n    fpr, tpr, _ = roc_curve(actuals, predicts, pos_label=1)\n    a = auc(fpr, tpr)\n\n    # Gerando o relatório de classificação\n    r = classification_report(actuals, predicts, zero_division=1)\n\n    # Calculando a matriz de confusão\n    c = confusion_matrix(actuals, predicts)\n    accuracy = np.trace(c)/np.sum(c)\n    \n    # Validação\n    # Fazendo previsões\n    predicts_v = model.predict(x_valid)\n    predicts_v = predicts_v.argmax(axis=1)\n\n    # Obtendo os rótulos verdadeiros\n    actuals_v=y_valid.argmax(axis=1)\n    \n    # Calculando a matriz de confusão\n    c_v = confusion_matrix(actuals, predicts)\n    accuracy_v = np.trace(c)/np.sum(c)\n\n    lst_history.append(history)\n    lst_times.append(training_time)\n    lst_accuracy.append(accuracy)\n    lst_acc.append(test_acc)\n    lst_loss.append(test_loss)\n    lst_AUC.append(a)\n    lst_reports.append(r)\n    lst_matrix.append(c)\n    \n    lst_accuracy_v.append(accuracy_v)\n    lst_matrix_v.append(c_v)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Salvamento de Dados","metadata":{}},{"cell_type":"code","source":"#--------------------------------------------------------------------------\n# SALVAMENTO DOS DADOS\n\nprint('\\nSalvando informações da rede......................................!')\n\npath = RESULTS_PATH + f'/{DATASET_TYPE}-resultados_{CNN_MODEL}-batch{BATCH_SIZE}.txt'\n    \nmatrix_total = np.sum(lst_matrix, axis=0)\naccuracy_total = np.trace(matrix_total)/np.sum(matrix_total)\n\nmatrix_total_v = np.sum(lst_matrix_v, axis=0)\naccuracy_total_v = np.trace(matrix_total_v)/np.sum(matrix_total_v)\n    \nlosses=[]\nval_losses=[]\naccuracies=[]\nval_accuracies=[]\n\nfor item in lst_history:\n    \n    history=item.history\n    loss=history['loss']\n    accuracy=history['categorical_accuracy']\n    \n    val_loss=history['val_loss']\n    val_accuracy=history['val_categorical_accuracy']\n    \n    losses.append(sum(loss)/len(loss))\n    accuracies.append(sum(accuracy)/len(accuracy))\n    \n    val_losses.append(sum(val_loss)/len(val_loss))\n    val_accuracies.append(sum(val_accuracy)/len(val_accuracy))\n\nf1=open(path,'w')\nf1.write(f'TREINAMENTO USANDO {CNN_MODEL}-batch{BATCH_SIZE} E DATASET {DATASET_TYPE}\\n')\n\nf1.write('\\nTest Accuracias: '+str(lst_acc)+'\\nTest Losses: '+str(lst_loss))\nf1.write('\\n\\nTest Accuracies Mean: '+str(np.mean(lst_acc)))\n\nf1.write('\\n\\n__________________________________________________________\\n')\n\nf1.write('\\n\\nValid Accuracies: '+str(val_accuracies)+'\\nValid Losses: '+str(val_losses))\nf1.write('\\n\\nValid Accuracies Mean: '+str(np.mean(val_accuracies)))\n\nf1.write('\\n\\n__________________________________________________________\\n')\n\nf1.write('\\nAccuracies from Confusion Matrix: '+str(lst_accuracy))\n\nf1.write('\\n\\nTotal Confusion Matrix: \\n'+str(matrix_total)+'\\n\\n')\nf1.write('\\nTotal Accuracie from Confusion Matrix: '+str(accuracy_total))\n\nf1.write('\\n\\n__________________________________________________________\\n')\n\nf1.write('\\n\\nTotal Confusion Matrix Valid: \\n'+str(matrix_total_v)+'\\n\\n')\nf1.write('\\nTotal Accuracie from Confusion Matrix Valid: '+str(accuracy_total_v))\n\nf1.write('\\n\\n__________________________________________________________\\n')\n\nf1.write('\\n\\nMetrics for all Folds: \\n\\n')\nfor i in range(len(lst_reports)):\n    f1.write(str(lst_reports[i]))\n    f1.write('\\n\\nTraining Time: '+str(lst_times[i])+'\\nAUC: '+str(lst_AUC[i]))\n    f1.write('\\n\\nAcurácia: ' + str(lst_accuracy[i]))\n    f1.write('\\n\\nMatriz de Confusao: \\n'+str(lst_matrix[i])+'\\n\\n__________________________________________________________\\n')\nf1.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def NetPlot(net_histories,n_epch):\n    import numpy as np\n    import matplotlib.pyplot as plt\n\n    losses=[]\n    val_losses=[]\n    accuracies=[]\n    val_accuracies=[]\n\n    for item in net_histories:\n\n        history=item.history\n        loss=history['loss']\n        val_loss=history['val_loss']\n        accuracy=history['categorical_accuracy']\n        val_accuracy=history['val_categorical_accuracy']\n\n        losses.append(loss)\n        val_losses.append(val_loss)\n        accuracies.append(accuracy)\n        val_accuracies.append(val_accuracy)\n\n\n    losses2=np.zeros((1,n_epch))\n    val_losses2=np.zeros((1,n_epch))\n    accuracies2=np.zeros((1,n_epch))\n    val_accuracies2=np.zeros((1,n_epch))\n\n    for i in losses:\n        losses2+=i\n\n    for i in val_losses:\n        val_losses2+=i\n\n    for i in accuracies:\n        accuracies2+=i\n\n    for i in val_accuracies:\n        val_accuracies2+=i\n        \n    N_SPLITS = 5\n\n    # 10 is number of folds\n    losses2=(losses2/N_SPLITS).flatten()\n    accuracies2=(accuracies2/N_SPLITS).flatten()\n    val_losses2=(val_losses2/N_SPLITS).flatten()\n    val_accuracies2=(val_accuracies2/N_SPLITS).flatten()\n    \n    # print('\\nAccuracies: '+ str(accuracies2) +'\\nLosses: '+ str(losses2))\n\n    plt.figure('Accracy Diagram',dpi=600)\n    plt.title(f'Accracy Diagram - {CNN_MODEL}-batch{BATCH_SIZE} - {DATASET_TYPE}')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.plot(accuracies2,color='black')\n    plt.plot(val_accuracies2,color='green')\n    plt.legend(['Train Data','Validation Data'])\n    plt.savefig(f'{RESULTS_PATH}/Accuracy-Diagram-{CNN_MODEL}-batch{BATCH_SIZE}.jpg')\n\n    plt.figure('Loss Diagram',dpi=600)\n    plt.title(f'Loss Diagram - {CNN_MODEL}-batch{BATCH_SIZE} - {DATASET_TYPE}')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.plot(losses2,color='black')\n    plt.plot(val_losses2,color='green')\n    plt.legend(['Train Data','Validation Data'])\n    plt.savefig(f'{RESULTS_PATH}/Loss-Diagram-{CNN_MODEL}-batch{BATCH_SIZE}.jpg')\n    \nNetPlot(lst_history,N_EPOCHS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Zipando pasta com resultados\nimport zipfile\n\ndef compactar_pasta(pasta, nome_arquivo_zip):\n    with zipfile.ZipFile(nome_arquivo_zip, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n        for root, _, files in os.walk(pasta):\n            for arquivo in files:\n                caminho_completo = os.path.join(root, arquivo)\n                zip_file.write(caminho_completo, os.path.relpath(caminho_completo, pasta))\n\nnome_arquivo_zip = f'{DATASET_TYPE}-{CNN_MODEL}-batch{BATCH_SIZE}-results.zip'\ncompactar_pasta(RESULTS_PATH, nome_arquivo_zip)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
