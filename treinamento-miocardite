{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8733110,"sourceType":"datasetVersion","datasetId":5242137},{"sourceId":8740562,"sourceType":"datasetVersion","datasetId":5242143,"isSourceIdPinned":true}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/robertgvds/treinamento-myocarditis?scriptVersionId=185447261\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# TREINAMENTO USANDO A REDE INICIAL CATEGORICA USANDO CONV2 COM DADOS EM ARRAYS","metadata":{}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-06-25T21:37:58.646095Z","iopub.execute_input":"2024-06-25T21:37:58.646538Z","iopub.status.idle":"2024-06-25T21:37:59.69454Z","shell.execute_reply.started":"2024-06-25T21:37:58.646505Z","shell.execute_reply":"2024-06-25T21:37:59.693493Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Tue Jun 25 21:37:59 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   40C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\n| N/A   38C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.environ['TF_DISABLE_JIT'] = '1'","metadata":{"execution":{"iopub.status.busy":"2024-06-25T21:37:59.696939Z","iopub.execute_input":"2024-06-25T21:37:59.697326Z","iopub.status.idle":"2024-06-25T21:37:59.702491Z","shell.execute_reply.started":"2024-06-25T21:37:59.697289Z","shell.execute_reply":"2024-06-25T21:37:59.701438Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!jupyter notebook --NotebookApp.iopub_msg_rate_limit=1.0e10","metadata":{"execution":{"iopub.status.busy":"2024-06-25T21:37:59.703956Z","iopub.execute_input":"2024-06-25T21:37:59.704215Z","iopub.status.idle":"2024-06-25T21:38:05.186254Z","shell.execute_reply.started":"2024-06-25T21:37:59.704195Z","shell.execute_reply":"2024-06-25T21:38:05.185289Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[32m[I 21:38:02.165 NotebookApp]\u001b[m [nb_conda_kernels] enabled, 1 kernels found\n\n  _   _          _      _\n | | | |_ __  __| |__ _| |_ ___\n | |_| | '_ \\/ _` / _` |  _/ -_)\n  \\___/| .__/\\__,_\\__,_|\\__\\___|\n       |_|\n                       \nRead the migration plan to Notebook 7 to learn about the new features and the actions to take if you are using extensions.\n\nhttps://jupyter-notebook.readthedocs.io/en/latest/migrate_to_notebook7.html\n\nPlease note that updating to Notebook 7 might break some of your extensions.\n\n\u001b[32m[I 21:38:02.502 NotebookApp]\u001b[m Registered dataproc_jupyter_plugin server extension\njupyter_http_over_ws extension initialized. Listening on /http_over_websocket\n\u001b[32m[I 21:38:02.776 NotebookApp]\u001b[m Skipped non-installed server(s): bash-language-server, dockerfile-language-server-nodejs, javascript-typescript-langserver, jedi-language-server, julia-language-server, pyright, python-language-server, r-languageserver, sql-language-server, texlab, typescript-language-server, unified-language-server, vscode-css-languageserver-bin, vscode-html-languageserver-bin, vscode-json-languageserver-bin, yaml-language-server\n\u001b[32m[I 21:38:03.185 NotebookApp]\u001b[m [Jupytext Server Extension] Deriving a JupytextContentsManager from LargeFileManager\n\u001b[32m[I 21:38:03.460 NotebookApp]\u001b[m [nb_conda] enabled\n\u001b[35m[C 21:38:03.523 NotebookApp]\u001b[m You must use Jupyter Server v1 to load nbdime as a classic notebook server extension. You have v2.12.5 installed.\n    You can fix this by executing:\n        pip install -U \"jupyter-server<2.0.0\"\n\u001b[35m[C 21:38:04.794 NotebookApp]\u001b[m Running as root is not recommended. Use --allow-root to bypass.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Bibliotecas e Constantes","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow[and-cuda]","metadata":{"execution":{"iopub.status.busy":"2024-06-25T21:38:05.188627Z","iopub.execute_input":"2024-06-25T21:38:05.188944Z","iopub.status.idle":"2024-06-25T21:45:42.131811Z","shell.execute_reply.started":"2024-06-25T21:38:05.188916Z","shell.execute_reply":"2024-06-25T21:45:42.130119Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow[and-cuda] in /opt/conda/lib/python3.10/site-packages (2.15.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (0.2.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (4.9.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.59.3)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (2.15.1)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (2.15.0)\nCollecting keras<2.16,>=2.15.0 (from tensorflow[and-cuda])\n  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\nCollecting nvidia-cublas-cu12==12.2.5.6 (from tensorflow[and-cuda])\n  Downloading nvidia_cublas_cu12-12.2.5.6-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.2.142 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_cupti_cu12-12.2.142-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cuda-nvcc-cu12==12.2.140 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_nvcc_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.2.140 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_nvrtc_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.2.140 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_runtime_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cudnn-cu12==8.9.4.25 (from tensorflow[and-cuda])\n  Downloading nvidia_cudnn_cu12-8.9.4.25-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cufft-cu12==11.0.8.103 (from tensorflow[and-cuda])\n  Downloading nvidia_cufft_cu12-11.0.8.103-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.3.141 (from tensorflow[and-cuda])\n  Downloading nvidia_curand_cu12-10.3.3.141-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.5.2.141 (from tensorflow[and-cuda])\n  Downloading nvidia_cusolver_cu12-11.5.2.141-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.2.141 (from tensorflow[and-cuda])\n  Downloading nvidia_cusparse_cu12-12.1.2.141-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.16.5 (from tensorflow[and-cuda])\n  Downloading nvidia_nccl_cu12-2.16.5-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvjitlink-cu12==12.2.140 (from tensorflow[and-cuda])\n  Downloading nvidia_nvjitlink_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting tensorrt==8.6.1.post1 (from tensorflow[and-cuda])\n  Downloading tensorrt-8.6.1.post1.tar.gz (18 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting tensorrt-bindings==8.6.1 (from tensorflow[and-cuda])\n  Downloading tensorrt_bindings-8.6.1-cp310-none-manylinux_2_17_x86_64.whl.metadata (621 bytes)\nINFO: pip is looking at multiple versions of tensorflow[and-cuda] to determine which version is compatible with other requirements. This could take a while.\nCollecting tensorflow[and-cuda]\n  Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\nCollecting ml-dtypes~=0.3.1 (from tensorflow[and-cuda])\n  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (2.32.3)\nCollecting tensorboard<2.17,>=2.16 (from tensorflow[and-cuda])\n  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: keras>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (3.3.3)\nCollecting nvidia-cublas-cu12==12.3.4.1 (from tensorflow[and-cuda])\n  Downloading nvidia_cublas_cu12-12.3.4.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.3.101 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_cupti_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cuda-nvcc-cu12==12.3.107 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_nvcc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.3.107 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_nvrtc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.3.101 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_runtime_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cudnn-cu12==8.9.7.29 (from tensorflow[and-cuda])\n  Downloading nvidia_cudnn_cu12-8.9.7.29-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cufft-cu12==11.0.12.1 (from tensorflow[and-cuda])\n  Downloading nvidia_cufft_cu12-11.0.12.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.4.107 (from tensorflow[and-cuda])\n  Downloading nvidia_curand_cu12-10.3.4.107-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.5.4.101 (from tensorflow[and-cuda])\n  Downloading nvidia_cusolver_cu12-11.5.4.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.2.0.103 (from tensorflow[and-cuda])\n  Downloading nvidia_cusparse_cu12-12.2.0.103-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.19.3 (from tensorflow[and-cuda])\n  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvjitlink-cu12==12.3.101 (from tensorflow[and-cuda])\n  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow[and-cuda]) (0.42.0)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow[and-cuda]) (13.7.0)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow[and-cuda]) (0.0.8)\nRequirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow[and-cuda]) (0.11.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2024.2.2)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow[and-cuda]) (3.5.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow[and-cuda]) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow[and-cuda]) (3.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow[and-cuda]) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow[and-cuda]) (2.1.3)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow[and-cuda]) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow[and-cuda]) (2.17.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow[and-cuda]) (0.1.2)\nDownloading nvidia_cublas_cu12-12.3.4.1-py3-none-manylinux1_x86_64.whl (412.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.6/412.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (14.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvcc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl (22.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.0/22.0 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl (24.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (867 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.7/867.7 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.7.29-py3-none-manylinux1_x86_64.whl (704.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m704.7/704.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0mm\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.12.1-py3-none-manylinux1_x86_64.whl (98.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.8/98.8 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.4.107-py3-none-manylinux1_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.5.4.101-py3-none-manylinux1_x86_64.whl (125.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.2.0.103-py3-none-manylinux1_x86_64.whl (197.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.5/197.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-nvcc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ml-dtypes, tensorboard, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, tensorflow\n  Attempting uninstall: ml-dtypes\n    Found existing installation: ml-dtypes 0.2.0\n    Uninstalling ml-dtypes-0.2.0:\n      Successfully uninstalled ml-dtypes-0.2.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.15.1\n    Uninstalling tensorboard-2.15.1:\n      Successfully uninstalled tensorboard-2.15.1\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.15.0\n    Uninstalling tensorflow-2.15.0:\n      Successfully uninstalled tensorflow-2.15.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow-decision-forests 1.8.1 requires tensorflow~=2.15.0, but you have tensorflow 2.16.1 which is incompatible.\ntensorflow-text 2.15.0 requires tensorflow<2.16,>=2.15.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.16.1 which is incompatible.\ntf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed ml-dtypes-0.3.2 nvidia-cublas-cu12-12.3.4.1 nvidia-cuda-cupti-cu12-12.3.101 nvidia-cuda-nvcc-cu12-12.3.107 nvidia-cuda-nvrtc-cu12-12.3.107 nvidia-cuda-runtime-cu12-12.3.101 nvidia-cudnn-cu12-8.9.7.29 nvidia-cufft-cu12-11.0.12.1 nvidia-curand-cu12-10.3.4.107 nvidia-cusolver-cu12-11.5.4.101 nvidia-cusparse-cu12-12.2.0.103 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.3.101 tensorboard-2.16.2 tensorflow-2.16.1\nCollecting efficientnet\n  Downloading efficientnet-1.1.1-py3-none-any.whl.metadata (6.4 kB)\nCollecting keras-applications<=1.0.8,>=1.0.7 (from efficientnet)\n  Downloading Keras_Applications-1.0.8-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from efficientnet) (0.22.0)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.10/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.26.4)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (3.10.0)\nRequirement already satisfied: scipy>=1.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (1.11.4)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (3.2.1)\nRequirement already satisfied: pillow>=9.0.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (9.5.0)\nRequirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (2.33.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (2023.12.9)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (21.3)\nRequirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image->efficientnet) (3.1.1)\nDownloading efficientnet-1.1.1-py3-none-any.whl (18 kB)\nDownloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m899.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: keras-applications, efficientnet\nSuccessfully installed efficientnet-1.1.1 keras-applications-1.0.8\n","output_type":"stream"}]},{"cell_type":"code","source":"#------------------------------------------------------------------------------\n# BIBLIOTECAS\n\nimport os\nimport numpy as np\nfrom PIL import Image\nimport sys\nimport random\n\n# BIBLIOTECAS DEEP LEARNING\nimport datetime\nimport tensorflow as tf\nfrom sklearn.metrics import auc, classification_report, confusion_matrix, roc_curve\nfrom tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import CSVLogger\n\n#------------------------------------------------------------------------------\n# CONSTANTES\n\nDATASET_TYPE = ['cleaned', 'selected']\nDATASET_TYPE = DATASET_TYPE[0] # Dataset utilizado no treinamento\n\nSEED = 10\nnp.random.seed(SEED) # semente geradora dos numeros aleatorios\nrandom.seed(SEED)\ntf.random.set_seed(SEED)\n\nN_FOLDS = 5\nN_EPOCHS = 30\nBATCH_SIZE = (32 if DATASET_TYPE == 'selected' else 512)\nTARGET_SIZE = (100, 100)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T21:45:42.134075Z","iopub.execute_input":"2024-06-25T21:45:42.134458Z","iopub.status.idle":"2024-06-25T21:45:45.992212Z","shell.execute_reply.started":"2024-06-25T21:45:42.134418Z","shell.execute_reply":"2024-06-25T21:45:45.99139Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Diretórios e Carregamentos de Dados","metadata":{}},{"cell_type":"code","source":"# Criando pasta de resultados\nif not os.path.exists(f'{DATASET_TYPE}'):\n    os.mkdir(f'{DATASET_TYPE}')","metadata":{"execution":{"iopub.status.busy":"2024-06-25T21:45:45.995587Z","iopub.execute_input":"2024-06-25T21:45:45.996097Z","iopub.status.idle":"2024-06-25T21:45:46.000572Z","shell.execute_reply.started":"2024-06-25T21:45:45.996069Z","shell.execute_reply":"2024-06-25T21:45:45.999549Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#------------------------------------------------------------------------------\n# DIRETÓRIOS DOS DATASETS\n\nDATASET_PATH = f'/kaggle/input/myocardits-dataset-{DATASET_TYPE}'\nRESULTS_PATH = f'/kaggle/working/{DATASET_TYPE}'\n\nNORMAL_PATH = DATASET_PATH + '/Normal/'\nSICK_PATH = DATASET_PATH + '/Sick/'\n\n# Diretorios de cada Individuo:\nnormal_datasets = [f'{NORMAL_PATH}Individuo_{i:02}/' for i in range(1, 17)]\nsick_datasets = [f'{SICK_PATH}Individuo_{i:02}/' for i in range(1, 32)]\n\nprint(normal_datasets)\nprint(sick_datasets)\n\nNORMAL_SPLITS = [[9, 10, 12, 15],\n                 [1,8,11],\n                 [4,5,14],\n                 [2,7,13],\n                 [3,6,16]]\n\nSICK_SPLITS = [[21,23,26,27,36,38],\n               [18,37,39,40,41,45,46],\n               [20,24,28,29,31,32],\n               [19,22,30,33,42,47],\n               [17,25,34,35,43,44]]\n\nnormal_splits = [[],[],[],[],[]]\nsick_splits = [[],[],[],[],[]]\n\nfor split in range(N_FOLDS):\n    normal_splits[split].extend(normal_datasets[i-1] for i in NORMAL_SPLITS[split])\n    sick_splits[split].extend(sick_datasets[i-17] for i in SICK_SPLITS[split])","metadata":{"execution":{"iopub.status.busy":"2024-06-25T21:45:46.002096Z","iopub.execute_input":"2024-06-25T21:45:46.002459Z","iopub.status.idle":"2024-06-25T21:45:46.019265Z","shell.execute_reply.started":"2024-06-25T21:45:46.002421Z","shell.execute_reply":"2024-06-25T21:45:46.018369Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"['/kaggle/input/myocardits-dataset-cleaned/Normal/Individuo_01/', '/kaggle/input/myocardits-dataset-cleaned/Normal/Individuo_02/', '/kaggle/input/myocardits-dataset-cleaned/Normal/Individuo_03/', '/kaggle/input/myocardits-dataset-cleaned/Normal/Individuo_04/', '/kaggle/input/myocardits-dataset-cleaned/Normal/Individuo_05/', '/kaggle/input/myocardits-dataset-cleaned/Normal/Individuo_06/', '/kaggle/input/myocardits-dataset-cleaned/Normal/Individuo_07/', '/kaggle/input/myocardits-dataset-cleaned/Normal/Individuo_08/', '/kaggle/input/myocardits-dataset-cleaned/Normal/Individuo_09/', '/kaggle/input/myocardits-dataset-cleaned/Normal/Individuo_10/', '/kaggle/input/myocardits-dataset-cleaned/Normal/Individuo_11/', '/kaggle/input/myocardits-dataset-cleaned/Normal/Individuo_12/', '/kaggle/input/myocardits-dataset-cleaned/Normal/Individuo_13/', '/kaggle/input/myocardits-dataset-cleaned/Normal/Individuo_14/', '/kaggle/input/myocardits-dataset-cleaned/Normal/Individuo_15/', '/kaggle/input/myocardits-dataset-cleaned/Normal/Individuo_16/']\n['/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_01/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_02/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_03/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_04/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_05/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_06/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_07/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_08/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_09/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_10/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_11/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_12/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_13/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_14/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_15/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_16/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_17/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_18/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_19/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_20/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_21/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_22/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_23/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_24/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_25/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_26/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_27/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_28/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_29/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_30/', '/kaggle/input/myocardits-dataset-cleaned/Sick/Individuo_31/']\n","output_type":"stream"}]},{"cell_type":"code","source":"#------------------------------------------------------------------------------\n# CARREGAMENTO DE DADOS\n\nprint('\\nIniciando carregamento e processamento das imagens..............!')\n\ndef carregar_imagens(diretorio):\n    global num_imagens\n    imagens = []\n    for pasta_atual, subpastas, arquivos in os.walk(diretorio):\n        for arquivo in arquivos:\n            if arquivo.endswith(('.jpg', '.jpeg', '.png')):\n                caminho = os.path.join(pasta_atual, arquivo)\n                \n                img = Image.open(caminho)\n                img = img.resize(TARGET_SIZE)\n                img_array = np.array(img)\n                imagens.append(img_array)\n                \n                num_imagens += 1\n                sys.stdout.write(\"\\rNumero de imagens carregados: %i\" % num_imagens)\n                sys.stdout.flush()\n                \n    return imagens\n\nnum_imagens = 0\n\nprint('\\nPacientes normais:')\nnormal_groups = []\nfor diretorios in normal_splits:\n    imagens = []\n    for individuos in diretorios:\n        imagens.extend(carregar_imagens(individuos))\n    normal_groups.append(imagens)\n\nnum_imagens = 0\n\nprint('\\n\\nPacientes doentes:')\nsick_groups = []\nfor diretorios in sick_splits:\n    imagens = []\n    for individuos in diretorios:\n        imagens.extend(carregar_imagens(individuos))\n    sick_groups.append(imagens)\n\n# DATASETS SEPRADAOS EM 5 PARA VALIDAÇÃO CRUZADA    \nx_data = [[],[],[],[],[]]\ny_data = [[],[],[],[],[]]\n\nprint('\\n\\nNumero de imagens por split:')\nfor i in range(N_FOLDS):\n    x_data[i].extend(path for path in normal_groups[i])\n    y_data[i].extend([1, 0] for path in normal_groups[i])\n    x_data[i].extend(path for path in sick_groups[i])\n    y_data[i].extend([0, 1] for path in sick_groups[i])\n    print(f'Split {i+1}: {len(x_data[i])} imagens ({len(normal_groups[i])} saudáveis e {len(sick_groups[i])} doentes).')","metadata":{"execution":{"iopub.status.busy":"2024-06-25T21:45:46.020486Z","iopub.execute_input":"2024-06-25T21:45:46.020818Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\nIniciando carregamento e processamento das imagens..............!\n\nPacientes normais:\nNumero de imagens carregados: 36493","output_type":"stream"}]},{"cell_type":"markdown","source":"## Treinamento","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Arquitetura CNN\ndef cnn_model(input_shape, num_classes):\n    model=Sequential()\n    model.add(Conv2D(32,3,padding='same',activation='relu',strides=2,input_shape=input_shape))\n    model.add(Conv2D(64,3,padding='same',activation='relu',strides=2))\n    model.add(Conv2D(128,3,padding='same',activation='relu',strides=2))\n    model.add(Conv2D(256,3,padding='same',activation='relu',strides=1))\n    model.add(Conv2D(256,3,padding='same',activation='relu',strides=1))\n    model.add(Conv2D(256,3,padding='same',activation='relu',strides=1))\n    model.add(Flatten())\n    model.add(Dense(256,activation='relu'))\n    model.add(Dense(128,activation='relu'))\n    model.add(Dense(64,activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(num_classes,activation='softmax'))\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#------------------------------------------------------------------------------\n# SEPARAÇÃO DE DADOS E TREINAMENTO\n\nlst_accuracy=[]\nlst_acc=[]\nlst_loss=[]\nlst_reports=[]\nlst_AUC=[]\nlst_matrix=[]\nlst_times=[]\nlst_history=[]\n\n#------------------------------------------------------------------------------\n# TREINAMENTO POR FOLDS\n\nfor fold in range(N_FOLDS):\n\n    print(f'\\n\\nFOLD {fold+1}:')\n\n    #--------------------------------------------------------------------------\n    # CARREGAMENTO DAS IMAGENS DE TREINAMENTO, VALIDACAO E TESTE\n\n    print(f'\\nCarregamento das imagens do fold {fold+1} para treinamento.............!')\n\n    folds = [0, 1, 2, 3, 4]\n\n    x_test = np.array(x_data[fold])\n    y_test = np.array(y_data[fold])\n    folds.remove(fold)\n\n    x_valid = np.array(x_data[folds[0]])\n    y_valid = np.array(y_data[folds[0]])\n    folds.remove(folds[0])\n\n    x_train, y_train = [], []\n    for i in folds:\n        x_train.extend(x_data[i])\n        y_train.extend(y_data[i])\n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n\n    print(f'Numero de imagens no treinamento: {len(x_train)} imagens.')\n    print(f'Numero de imagens na validação: {len(x_valid)} imagens.')\n    print(f'Numero de imagens no teste: {len(x_test)} imagens.')\n\n    #--------------------------------------------------------------------------\n    # ARQUITETURA E COMPILACAO\n\n    model=cnn_model((100, 100, 1), 2)\n    \n    # Compilacao do modelo\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n\n    #--------------------------------------------------------------------------\n    # TREINAMENTO\n    print('\\nIniciando o treinamento.........................................!\\n')\n\n    calback=CSVLogger(RESULTS_PATH + f'/logger_fold{fold+1}.log')\n\n    # Treinando o modelo\n    start=datetime.datetime.now()\n\n    history=model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=N_EPOCHS, validation_data=(x_valid, y_valid), callbacks=[calback])\n\n    end=datetime.datetime.now()\n    training_time=end-start\n\n    # Salvamento do modelo\n    model.save(RESULTS_PATH + f'/{DATASET_TYPE}-CNN-{fold+1}.h5')\n\n    #--------------------------------------------------------------------------\n    # TESTE\n\n    # Testando o modelo\n    print(\"\\nTestando imagens................................................!\\n\")\n\n    # Acuracia e Perda do Teste\n    test_loss, test_acc = model.evaluate(x_test, y_test)\n\n    print(model.metrics_names)\n\n    #--------------------------------------------------------------------------\n    # ARMAZENAMENTO DOS INFORMACOES\n\n    # Fazendo previsões\n    predicts = model.predict(x_test)\n    predicts = predicts.argmax(axis=1)\n\n    # Obtendo os rótulos verdadeiros\n    actuals=y_test.argmax(axis=1)\n\n    # Calculando a curva ROC\n    fpr, tpr, _ = roc_curve(actuals, predicts, pos_label=1)\n    a = auc(fpr, tpr)\n\n    # Gerando o relatório de classificação\n    r = classification_report(actuals, predicts, zero_division=1)\n\n    # Calculando a matriz de confusão\n    c = confusion_matrix(actuals, predicts)\n    accuracy = np.trace(c)/np.sum(c)\n\n    lst_history.append(history)\n    lst_times.append(training_time)\n    lst_accuracy.append(accuracy)\n    lst_acc.append(test_acc)\n    lst_loss.append(test_loss)\n    lst_AUC.append(a)\n    lst_reports.append(r)\n    lst_matrix.append(c)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Salvamento de Dados","metadata":{}},{"cell_type":"code","source":"#--------------------------------------------------------------------------\n# SALVAMENTO DOS DADOS\n\nprint('\\nSalvando informações da rede......................................!')\n\npath = RESULTS_PATH + f'/{DATASET_TYPE}-resultados_rede-incial-conv2d.txt'\n    \nmatrix_total = np.sum(lst_matrix, axis=0)\naccuracy_total = np.trace(matrix_total)/np.sum(matrix_total)\n    \nlosses=[]\nval_losses=[]\naccuracies=[]\nval_accuracies=[]\n\nfor item in lst_history:\n    \n    history=item.history\n    loss=history['loss']\n    accuracy=history['categorical_accuracy']\n    \n    val_loss=history['val_loss']\n    val_accuracy=history['val_categorical_accuracy']\n    \n    losses.append(sum(loss)/len(loss))\n    accuracies.append(sum(accuracy)/len(accuracy))\n    \n    val_losses.append(sum(val_loss)/len(val_loss))\n    val_accuracies.append(sum(val_accuracy)/len(val_accuracy))\n\nf1=open(path,'w')\nf1.write(f'TREINAMENTO USANDO A REDE INICIAL CATEGORICA COM DADOS EM ARRAYS E DATASET {DATASET_TYPE}\\n')\n\nf1.write('\\nTest Accuracias: '+str(lst_acc)+'\\nTest Losses: '+str(lst_loss))\nf1.write('\\n\\nTest Accuracies Mean: '+str(np.mean(lst_acc)))\n\nf1.write('\\n\\n__________________________________________________________\\n')\n\nf1.write('\\n\\nValid Accuracies: '+str(val_accuracies)+'\\nValid Losses: '+str(val_losses))\nf1.write('\\n\\nValid Accuracies Mean: '+str(np.mean(val_accuracies)))\n\nf1.write('\\n\\n__________________________________________________________\\n')\n\nf1.write('\\nAccuracies from Confusion Matrix: '+str(lst_accuracy))\n\nf1.write('\\n\\nTotal Confusion Matrix: \\n'+str(matrix_total)+'\\n\\n')\nf1.write('\\nTotal Accuracie from Confusion Matrix: '+str(accuracy_total))\n\nf1.write('\\n\\n__________________________________________________________\\n')\n\nf1.write('\\n\\nMetrics for all Folds: \\n\\n')\nfor i in range(len(lst_reports)):\n    f1.write(str(lst_reports[i]))\n    f1.write('\\n\\nTraining Time: '+str(lst_times[i])+'\\nAUC: '+str(lst_AUC[i]))\n    f1.write('\\n\\nAcurácia: ' + str(lst_accuracy[i]))\n    f1.write('\\n\\nMatriz de Confusao: \\n'+str(lst_matrix[i])+'\\n\\n__________________________________________________________\\n')\nf1.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Zipando pasta com resultados\nimport zipfile\n\ndef compactar_pasta(pasta, nome_arquivo_zip):\n    with zipfile.ZipFile(nome_arquivo_zip, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n        for root, _, files in os.walk(pasta):\n            for arquivo in files:\n                caminho_completo = os.path.join(root, arquivo)\n                zip_file.write(caminho_completo, os.path.relpath(caminho_completo, pasta))\n\nnome_arquivo_zip = f'{DATASET_TYPE}-results.zip'\ncompactar_pasta(RESULTS_PATH, nome_arquivo_zip)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}